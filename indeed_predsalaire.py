# -*- coding: utf-8 -*-
"""Indeed_PredSalaire

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dkpLaus1QLThx3IjkMIqNhpT1a_vV0f0
"""

import pandas as pd
from pandas import json_normalize
import re
import numpy as np
import string
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('https://raw.githubusercontent.com/Tim-IA/test/main/preprocessing_emploi.csv')

df.head()

df.drop(['Unnamed: 0'], axis=1, inplace=True)

df.isna().sum()

df.info()

df[['poste', 'Departement', 'Contrat']] = df[['poste', 'Departement', 'Contrat']].astype("category")
df[['bac+', 'Statut', 'experience','dureeTravail_annuelle']]=df[['bac+', 'Statut', 'experience','dureeTravail_annuelle']].astype(int)

df.describe()

plt.figure(figsize=(15,10))
sns.heatmap(data=df.select_dtypes(include=['float64', 'int64']).dropna().corr(), annot=True, fmt='.1g')

fig, ax = plt.subplots(figsize=(10,5))
ax = sns.distplot(df['Salaire_Annuel'], bins=10)
ax.axvline(df['Salaire_Annuel'].mean(), c='r')
ax.axvline(df['Salaire_Annuel'].median(), c='g')
plt.show()

df.groupby(['poste', 'Contrat']).mean().round(2)

df['data scientist'] = np.where(df[['poste']].apply(lambda x: x.str.contains('data scientist',
                                                 case=False, regex=True)).any(1), 1, 0)
df['data manager'] = np.where(df[['poste']].apply(lambda x: x.str.contains('data manager',
                                                 case=False, regex=True)).any(1), 1, 0)
df['developpeur big data'] = np.where(df[['poste']].apply(lambda x: x.str.contains('developpeur big data',
                                                 case=False, regex=True)).any(1), 1, 0)
df['data engineer'] = np.where(df[['poste']].apply(lambda x: x.str.contains('data engineer',
                                                 case=False, regex=True)).any(1), 1, 0)
df['data analyst'] = np.where(df[['poste']].apply(lambda x: x.str.contains('data analyst',
                                                 case=False, regex=True)).any(1), 1, 0)
df['CDI'] = np.where(df[['Contrat']].apply(lambda x: x.str.contains('cdi',
                                                 case=False, regex=True)).any(1), 1, 0)
df['CDD'] = np.where(df[['Contrat']].apply(lambda x: x.str.contains('cdd',
                                                 case=False, regex=True)).any(1), 1, 0)
df['Intérim'] = np.where(df[['Contrat']].apply(lambda x: x.str.contains('mis',
                                                 case=False, regex=True)).any(1), 1, 0)

df.drop(['poste', 'Contrat'], axis=1, inplace=True)

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import SGDRegressor
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

df.head()

X = df.drop(columns='Salaire_Annuel')
y = df['Salaire_Annuel']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
print("The length of the initial dataset is :", len(X))
print("The length of the train dataset is   :", len(X_train))
print("The length of the test dataset is    :", len(X_test))

from sklearn.datasets import make_regression
from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor(random_state=0)
model.fit(X_train, y_train)
model.score(X_train, y_train)

from sklearn.metrics import mean_squared_error, r2_score, median_absolute_error, mean_absolute_error
prediction = model.predict(X_test)
r2_score(y_test, prediction)
mean_absolute_error(y_test, prediction)
median_absolute_error(y_test, prediction)
mean_squared_error(y_test, prediction)


print("R2:", round(r2_score(y_test, prediction), 2))
print("MAE(mean):", round(mean_absolute_error(y_test, prediction), 2))
print("MAE(median):", round(median_absolute_error(y_test, prediction), 2))
print("RMSE:", round(np.sqrt(mean_squared_error(y_test, prediction)), 2))

import pickle 
# Enregistrer le modèle
pickle.dump(model, open('model.pkl', 'wb'))





data = {'metier': ['Data Analyst'], 'Departement':[37], 'experience': [5], 'contract':['CDI'], 
        'Description': ['je cherche python anglais bigdata sql mongodb' ]}  
  
# Create DataFrame  
df_Job = pd.DataFrame(data)  
  
# Print the output.  
print(df_Job)
df_Job.columns

class PreprocessJob():
    
    ListDB = ['sql','postgresql','mysql','nosq','mongodb']
    ListLangProg = ['python','java','flutter', 'julia', 'rstudio', 'scala', 'sas', 'c++']
    ListLangWeb = ['javascript','php','css', 'html']
    ListBI = ['powerbi','power bi','tableau', 'sisense','qlik','sisense','analytics']
   
    def __init__(self,df):
        
        self.df_ = df
        
        self.df_['Description'] = self.df_['Description'].apply(self.text_cleaning_encoding)
        
        self.df_['Statut'] = self.df_['metier'].map(self.isStatut)
        self.df_['dureeTravail_annuelle'] = self.df_['contract'].map(self.isDureT)
        
        self.df_['bac+'] = self.df_['metier'].map(self.isBac)
        self.df_['Anglais'] = self.df_['Description'].map(self.isAnglais)
        self.df_['langages_programmation'] = self.df_['Description'].map(self.isLangProg)
        self.df_['langages_Web'] = self.df_['Description'].map(self.isLangWeb)
        self.df_['bases_donnees'] = self.df_['Description'].map(self.isDB)
        self.df_['BI_soft'] = self.df_['Description'].map(self.isBI_soft)
        self.df_['data scientist'] = self.df_['metier'].map(self.isDataScientist)
        self.df_['data manager'] = self.df_['metier'].map(self.isDataManager)
        self.df_['developpeur big data'] = self.df_['metier'].map(self.isDevBigData)
        self.df_['data engineer'] = self.df_['metier'].map(self.isDataEngineer)
        self.df_['data analyst'] = self.df_['metier'].map(self.isDataAnalyst)
        self.df_['CDI'] = self.df_['contract'].map(self.isCDI)
        self.df_['CDD'] = self.df_['contract'].map(self.isCDD)
        self.df_['Intérim'] = self.df_['contract'].map(self.isInterim)

        
        self.X = self.df_.drop(columns=['metier', 'contract', 'Description'])
        #self.y = self.df_['Salaire']
        
    def text_cleaning_encoding(self,text):
        text = text.lower() #Converting text into lowercase
        text = re.sub('h/f','',text) #Removing h/f
        text = re.sub('\(', '', text) #Removing )
        text = re.sub('\)', '', text) #Removing (
        text = re.sub('f/h','',text) #Removing f/h
        text = re.sub('it','',text) #Removing it
        text = re.sub("\n",'',text) #Removing new lines
        return(text)

    def isCDI(self,txt):
        if bool(re.search("CDI", txt)):
            return 1
        else:
            return 0
    
    def isCDD(self,txt):
        if bool(re.search("CDD", txt)):
            return 1
        else:
            return 0
        
    def isInterim(self,txt):
        if bool(re.search("Interim", txt)):
            return 1
        else:
            return 0
    
    def isDataScientist(self,txt):
        if bool(re.search("(?=.*scientist)(?=.*data)", txt)):
            return 1
        else:
            return 0

    def isDataAnalyst(self,txt):
        if bool(re.search("(?=.*Data)(?=.*Analyst)", txt)):
            return 1
        else:
            return 0
        
    def isDataEngineer(self,txt):
        if bool(re.search("(?=.*Data)(?=.*Engineer)", txt)):
            return 1
        else:
            return 0
        
    def isDataManager(self,txt):
        if bool(re.search("(?=.*Data)(?=.*Manager)", txt)):
            return 1
        else:
            return 0
        
    def isDevBigData(self,txt):
        if bool(re.search("(?=.*Developpeur)(?=.*BigData)", txt)):
            return 1
        else:
            return 0
        
    def isAnglais(self,txt):
        if bool(re.search("(?=.*anglais)", txt)):
            return 1
        else:
            return 0
    
    def isLangProg(self, txt):        
        for PreprocessJob.ListLangProg in txt:
            return 1
        else:
            return 0
    
    def isLangWeb(self, txt):        
        for PreprocessJob.ListLangWeb in txt:
            return 1
        else:
            return 0
    
    def isDB(self, txt):        
        for PreprocessJob.ListDB in txt:
            return 1
        else:
            return 0
    
    def isBI_soft(self, txt):        
        for PreprocessJob.ListBI in txt:
            return 1
        else:
            return 0
    
    def isBac(self,txt):
        if txt == 'Developpeur BigData':
            return 5
        elif txt == "Data Scientist":
            return 5
        elif txt == "Data Analyst":
            return 4
        elif txt == "Data Engineer":
            return 5
        elif txt == "Data Manager":
            return 3
        
    def isStatut(self,txt):
        if txt == 'Developpeur BigData':
            return 9
        elif txt == "Data Scientist":
            return 9
        elif txt == "Data Analyst":
            return 7
        elif txt == "Data Engineer":
            return 9
        elif txt == "Data Manager":
            return 8
    
    def isDureT(self,txt):
        if txt == 'CDI':
            return 1540
        else:
            return 1606
        
    def isPython(self,txt):
        if bool(re.search("python", txt)):
            return 1
        else:
            return 0
    
    def isJava(self,txt):
        if bool(re.search("java", txt)):
            return 1
        else:
            return 0

    def isSql(self,txt):
        if bool(re.search("sql", txt)):
            return 1
        else:
            return 0    
    
    def getDf(self):
        return self.df_
        
    def getFeatures(self):
        return self.X



preproc1 = PreprocessJob(df_Job)
df = preproc1.getDf()
X = preproc1.getFeatures()

X.detail

model.predict(X)









X_test

X_test['dureeTravail_annuelle'].unique()

X_test.columns

qcv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
pca=PCA()
grd=GradientBoostingRegressor()
lr=LinearRegression()
rf=RandomForestRegressor()
knn=KNeighborsRegressor()

column_cat = ['Departement']
transfo_cat = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
preparation = ColumnTransformer(transformers=[('data_cat', transfo_cat , column_cat)])

X_pca = pca.fit_transform(X)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

X_pca = pca.fit_transform(X_train_scaled)

total_explained_variance = pca.explained_variance_ratio_.cumsum()
n_over_95 = len(total_explained_variance[total_explained_variance >= .95])

n_to_reach_95 = X.shape[1]-n_over_95 + 1

print('Number features:{}\tTotal Variance Explained:{}'.format(n_to_reach_95, total_explained_variance[n_to_reach_95-1]))

pipe = Pipeline(steps=[('preparation', preparation),('scaler', StandardScaler()),('model', model)])

pipe = Pipeline(steps=[('preparation', preparation), ('pca', PCA(n_components=n_to_reach_95)),('scaler', StandardScaler()),('model', model)])

pipe.fit(X_train,y_train)

def perfomance(model):
    pipe = Pipeline(steps=[('preparation', preparation),('scaler', StandardScaler()),('model', model)])
#Les hyper-paramètres à définir pour knn: nb des voisins entre 1 et 15
    if model==knn:
        parameters = {'model__n_neighbors': range(1, 15, 1)} 
#Les hyper-paramètres à définir pour svm: type de noyau entre linéaire et Radial Basis Function( les 
                    #valeurs faibles signifiant « loin » et les valeurs élevées signifiant « proche »)
    elif model==grd:
        parameters = {'model__n_estimators':range(1, 120, 30),
                      'model__min_samples_split': range(2,10, 1)}
#Les hyper-paramètres à définir pour Random Forest: nb des arbres entre 1 et 220 avec pas de 40
                                                    #minimum d'échantillons requis pour séparer un nœud interne
    elif model==lr:
        parameters = {'model__normalize':(True, False)}
        
    elif model==rf:
        parameters = {'model__n_estimators':range(1, 120, 30),
                      'model__min_samples_split': range(2,10, 1)}
                    
    grid = GridSearchCV(pipe, parameters, cv = cv, scoring='r2')
    grid.fit(X_train, y_train)
    print('Le meilleur score:', round(grid.best_score_, 2))
    print('Les meilleurs paramètres:', grid.best_params_)
    y_pred = grid.predict(X_test)
    print("Coefficient de determination :", round(r2_score(y_test, y_pred), 2))
    print("RMSE :", round(np.sqrt(mean_squared_error(y_test, y_pred)), 2))
    print("MAE :", round(mean_absolute_error(y_test, y_pred), 2))

perfomance(rf)

corr = df.corr().round(3)
mask = np.zeros_like(corr)
mask[np.triu_indices_from(mask)] = True
fig, ax = plt.subplots(figsize=(14,10))
ax = sns.heatmap(corr, annot=True, fmt=".2f", annot_kws={'size':8}, 
                 mask=mask, 
                 center=0, cmap="coolwarm")
plt.title(f"Heatmap des corrélations linéaires\n", color='black')
plt.show()

