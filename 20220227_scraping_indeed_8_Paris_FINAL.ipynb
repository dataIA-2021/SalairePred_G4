{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6792f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "# connexion avec la base de donn√©es\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a49e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL = 'https://fr.indeed.com/jobs?q=data%20scientist&l=Paris%20(75)&start=20&vjk=3d5487bc57a59ba4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14e9f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_template = \"https://fr.indeed.com/jobs?q={}&l={}&start={}&sort=date\"\n",
    "max_results_per_city =10 # Set this to a high-value (5000) to generate more results. \n",
    "\n",
    "sleep_time = random.randrange(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e8a0386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 100 results. 92 of these aren't rubbish.\n",
      "You have 200 results. 183 of these aren't rubbish.\n",
      "You have 300 results. 254 of these aren't rubbish.\n",
      "You have 400 results. 304 of these aren't rubbish.\n",
      "You have 500 results. 383 of these aren't rubbish.\n",
      "You have 600 results. 441 of these aren't rubbish.\n",
      "You have 700 results. 526 of these aren't rubbish.\n",
      "You have 800 results. 595 of these aren't rubbish.\n",
      "You have 900 results. 694 of these aren't rubbish.\n",
      "You have 1000 results. 780 of these aren't rubbish.\n",
      "You have 1100 results. 862 of these aren't rubbish.\n",
      "You have 1200 results. 910 of these aren't rubbish.\n",
      "You have 1300 results. 979 of these aren't rubbish.\n",
      "You have 1400 results. 1052 of these aren't rubbish.\n",
      "You have 1500 results. 1114 of these aren't rubbish.\n",
      "You have 1600 results. 1203 of these aren't rubbish.\n",
      "You have 1700 results. 1286 of these aren't rubbish.\n",
      "You have 1800 results. 1369 of these aren't rubbish.\n",
      "You have 1900 results. 1438 of these aren't rubbish.\n",
      "You have 2000 results. 1503 of these aren't rubbish.\n",
      "You have 2100 results. 1538 of these aren't rubbish.\n",
      "You have 2200 results. 1538 of these aren't rubbish.\n",
      "You have 2300 results. 1539 of these aren't rubbish.\n",
      "You have 2400 results. 1539 of these aren't rubbish.\n",
      "You have 2500 results. 1539 of these aren't rubbish.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "results = []\n",
    "df_more = pd.DataFrame(columns=[\"Title\",\"Location\",\"Company\",\"Rating\",\"Salary\", \"Synopsis\"])\n",
    "\n",
    "for metier in set(['developpeur', 'data+scientist', 'data+analyst', 'business+intelligence']):    \n",
    "    for start in range(0, 1000, 10):\n",
    "\n",
    "        # Grab the results from the request (as above)\n",
    "        url = url_template.format(metier, 'Paris', start)\n",
    "        # Append to the full set of results\n",
    "        html = requests.get(url)\n",
    "        soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "        for each in soup.find_all(class_= \"result\" ):\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "            try:\n",
    "                title = each.find('span', title=True).text.replace('\\n', '')\n",
    "            except:\n",
    "                title = 'None'\n",
    "            try:\n",
    "                location = each.find('div', {'class':\"companyLocation\" }).text.replace('\\n', '')\n",
    "            except:\n",
    "                location = 'None'\n",
    "            try:\n",
    "                company = each.find(class_='companyName').text.replace('\\n', '')\n",
    "            except:\n",
    "                company = 'None'\n",
    "            try:\n",
    "                salary = each.find('div', {'class':'salary-snippet'}).text\n",
    "            except:\n",
    "                salary = 'None'\n",
    "            try:\n",
    "                rating_span = each.find('span', attrs={'class':  'ratingNumber'})\n",
    "                rating = float(rating_span.text.strip().replace(',', '.'))\n",
    "            except:\n",
    "                rating = 'None'\n",
    "            try:\n",
    "                if each.find('a',href=True):     \n",
    "\n",
    "                    jk=each['data-jk']\n",
    "                    #print(jk)\n",
    "                    #https://fr.indeed.com/voir-emploi?jk=006167bb602a6f9b\n",
    "                    url='https://fr.indeed.com/voir-emploi?jk='+(jk)\n",
    "                    #print(url)\n",
    "\n",
    "\n",
    "                    job_response = requests.get(url)\n",
    "                    job_soup = BeautifulSoup(job_response.content, 'html.parser')\n",
    "                    synopsis = job_soup.find('div', {'class' : 'jobsearch-jobDescriptionText'}).text\n",
    "            except:\n",
    "                synopsis = None\n",
    "\n",
    "            df_more = df_more.append({'Title':title, 'Location':location, 'Company':company, 'Rating':rating, 'Salary':salary, 'Synopsis':synopsis}, ignore_index=True)\n",
    "            i += 1\n",
    "            if i % 100 == 0:  # Ram helped me build this counter to see how many. You can visibly see Ram's vernacular in the print statements.\n",
    "                print('You have ' + str(i) + ' results. ' + str(df_more.dropna().drop_duplicates().shape[0]) + \" of these aren't rubbish.\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bcaf584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2541, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_more.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a7a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c0f4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more.to_csv('Indeed_Project_3_df_more_long_not_cleaned_paris.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f064e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
